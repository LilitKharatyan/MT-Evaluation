{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3sHeX7WNdktyMBeSQaPJn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LilitKharatyan/MT-Evaluation/blob/main/Arm_EnNMT_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting Bilingual Text from the Web-pages**"
      ],
      "metadata": {
        "id": "YU-W9GxuskW7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVDiUSkOr9KS"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import codecs\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url=\"https://www.mfa.am/en/speeches/\"\n",
        "f = urllib.request.urlopen(url)\n",
        "web=f.read().decode('utf-8')\n",
        "soup = BeautifulSoup(web, 'lxml')\n",
        "eng = []\n",
        "links = soup.find_all('a')\n",
        "for link in links:\n",
        "    link=link['href'] \n",
        "    if link.startswith(url):\n",
        "      text=[]\n",
        "      camps=link.split(\"/\")\n",
        "      if len(camps)>6:\n",
        "        eng.append(link)\n",
        "\n",
        "\n",
        "#out=codecs.open(\"en.txt\",\"w\",encoding=\"utf-8\")\n",
        "\n",
        "for i in eng:\n",
        "  print(\"I\",i)\n",
        "  #You should open a file for each web page. (You may prefer to put the files in different directories\n",
        "  outfilename=str(i.split(\"/\")[-1])+\"-en.txt\"\n",
        "  out=codecs.open(outfilename,\"w\",encoding=\"utf-8\")\n",
        "  h = urllib.request.urlopen(i)\n",
        "  web=h.read().decode('utf-8')\n",
        "  \n",
        "  soup = BeautifulSoup(web, 'lxml')\n",
        "\n",
        "  title = soup.find_all('strong')[0]\n",
        "  if not title==None:\n",
        "    title=title.getText()\n",
        "    print(title)\n",
        "    out.write(title+\"\\n\")\n",
        "\n",
        "  entry=soup.findAll(\"div\", {\"class\" : \"static-content\"})[0]\n",
        "  soup2 = BeautifulSoup(str(entry), 'lxml')\n",
        "  paras=soup2.find_all(\"p\")\n",
        "  for para in paras:\n",
        "    para=para.getText()\n",
        "    print(para)\n",
        "    out.write(para+\"\\n\")\n",
        "  out.close()\n",
        "\n",
        "#aut=codecs.open(\"arm.txt\",\"w\",encoding=\"utf-8\")\n",
        "\n",
        "for i in eng:\n",
        "  try:\n",
        "    print(\"I\",i)\n",
        "    \n",
        "    outfilename=str(i.split(\"/\")[-1])+\"-am.txt\"\n",
        "    out=codecs.open(outfilename,\"w\",encoding=\"utf-8\")\n",
        "    z = urllib.request.urlopen(i)\n",
        "    web=z.read().decode('utf-8')\n",
        "\n",
        "    soup = BeautifulSoup(web, 'lxml')\n",
        "\n",
        "    spanL2=soup.findAll(\"li\", {\"class\" : \"fl pr\"})\n",
        "    soup2 = BeautifulSoup(str(spanL2),'lxml')\n",
        "    linkL2=soup2.find_all(\"a\")[0]['href'] \n",
        "\n",
        "\n",
        "    arm = urllib.request.urlopen(linkL2)\n",
        "    web=arm.read().decode('utf-8')\n",
        "\n",
        "    soup = BeautifulSoup(web, 'lxml')\n",
        "\n",
        "    title = soup.find_all('strong')[0]\n",
        "    if not title==None:\n",
        "      title=title.getText()\n",
        "      print(title)\n",
        "      out.write(title+\"\\n\")\n",
        "\n",
        "    entry=soup.findAll(\"div\", {\"class\" : \"static-content\"})[0]\n",
        "    soup2 = BeautifulSoup(str(entry), 'lxml')\n",
        "    paras=soup2.find_all(\"p\")\n",
        "    for para in paras:\n",
        "      para=para.getText()\n",
        "      print(para)\n",
        "      out.write(para+\"\\n\")\n",
        "  except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting read of charchters between English and Armenian words**"
      ],
      "metadata": {
        "id": "g6SS0pECuFFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('baratian.tab.txt') as f:\n",
        "  lines = f.readlines()\n",
        "  for n in lines:\n",
        "    x = re.sub(\"\\(.*?\\)|\\[.*?\\]|\\ adv |\\ n |\\ v |\\ a |\\ n,|\\ I|\\ II |\\ III | 1.\",\"\", n)\n",
        "    print(x)\n",
        "\n",
        "\"\"\"**Getting only English and Armenian words**\"\"\"\n",
        "\n",
        "with open('baratian.tab.txt') as f:\n",
        "  lines = f.readlines()\n",
        "  for i in lines:\n",
        "    english_word = i.split()[0]\n",
        "    armenian_word = i.split()[3]\n",
        "    print(english_word, armenian_wor"
      ],
      "metadata": {
        "id": "eHaV5MiFuGVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Etracting English-Armenian Monoword Translations from a Tab-Delimited Text File**"
      ],
      "metadata": {
        "id": "kdjY9OGPuhuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "import re\n",
        "\n",
        "inputfile=codecs.open(\"baratian.tab.txt\",\"r\",encoding=\"utf-8\")\n",
        "outputfile=codecs.open(\"baratian-converted.txt\",\"w\",encoding=\"utf-8\")\n",
        "for linia in inputfile:\n",
        "    linia=linia.rstrip()\n",
        "    #We can separate English word from the rest using split with [ \n",
        "    fields=linia.split(\"[\")\n",
        "    english_word=fields[0].strip()\n",
        "    #We check whether the word is \"uniword\"\n",
        "    if len(english_word.split(\" \"))==1:\n",
        "        #now we split the line by n, a, v, adv\n",
        "        if linia.find(\"] n \")>-1: fields2=linia.split(\"] n \")\n",
        "        elif linia.find(\"] a \")>-1: fields2=linia.split(\"] a \")\n",
        "        elif linia.find(\"] v \")>-1: fields2=linia.split(\"] v \")\n",
        "        elif linia.find(\"] adv \")>-1: fields2=linia.split(\"] adv \")\n",
        "        #the Armenian words are in the second place of this split, but there can be different words and other information separated by ,\n",
        "        fields3=fields2[1].split(\",\")\n",
        "        #we check whether the Armenian word is \"monoword\"\n",
        "        for armenian_word in fields3:\n",
        "            armenian_word=armenian_word.strip()\n",
        "            if len(armenian_word.split(\" \"))==1:\n",
        "                cadena1=english_word+\"\\t\"+armenian_word\n",
        "                print(cadena1)\n",
        "                outputfile.write(cadena1+\"\\n\")"
      ],
      "metadata": {
        "id": "jb3DwdFAug6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Automated Script for Creating Alignment Script for Hunalign Using Source and Target Language Text Files and Bilingual Dictionary**"
      ],
      "metadata": {
        "id": "MlXAzujMs7F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(description='A script to create the align script to be used with hunalign.')\n",
        "parser.add_argument(\"--dirSL\", type=str, help=\"The input dir containing the segmented text files for the source language.\", required=True)\n",
        "parser.add_argument(\"--dirTL\", type=str, help=\"The input dir containing the segmented text files for the target language.\", required=True)\n",
        "parser.add_argument(\"--dirALI\", type=str, help=\"The output dir to save the aligned files.\", required=True)\n",
        "parser.add_argument(\"--dictionary\", type=str, help=\"The bilingual dictionary to use.\", required=True)\n",
        "parser.add_argument(\"--script\", type=str, help=\"The name of the alignment script.\", required=True)\n",
        "parser.add_argument(\"--r1\", type=str, help=\"The first string for name replacement.\", required=False)\n",
        "parser.add_argument(\"--r2\", type=str, help=\"The second string for name replacement.\", required=False)\n",
        "parser.add_argument(\"--windows\", action='store_true', help=\"Create a bat file for Windows.\", required=False)\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "windows=False\n",
        "if args.windows:\n",
        "    windows=True\n",
        "if args.r1:\n",
        "    r1=args.r1\n",
        "else:\n",
        "    r1=\"\"\n",
        "if args.r2:\n",
        "    r2=args.r2\n",
        "else:\n",
        "    r2=\"\"\n",
        "\n",
        "\n",
        "dir1=args.dirSL\n",
        "dir2=args.dirTL\n",
        "dir3=args.dirALI\n",
        "if not os.path.exists(dir3):\n",
        "    os.makedirs(dir3)\n",
        "outfile=args.script\n",
        "hundict=args.dictionary\n",
        "\n",
        "thisdir = os.getcwd()\n",
        "files1=[]\n",
        "for r, d, f in os.walk(dir1):\n",
        "    for file in f:\n",
        "        files1.append(file)\n",
        "        \n",
        "files2=[]\n",
        "for r, d, f in os.walk(dir2):\n",
        "    for file in f:\n",
        "        files2.append(file)\n",
        "sortida=codecs.open(outfile,\"w\",encoding=\"utf-8\")\n",
        "for file1 in files1:\n",
        "    file2=file1.replace(r1,r2)\n",
        "    if file2 in files2:\n",
        "        fileali=\"ali-\"+file1\n",
        "        if windows:\n",
        "            cadena=\"hunalign.exe \"+hundict+\" -utf -realign -text \\\"\"+dir1+\"/\"+file1+\"\\\" \"+\"\\\"\"+dir2+\"/\"+file2+\"\\\" > \\\"\"+dir3+\"/\"+fileali+\"\\\"\"\n",
        "        else:\n",
        "            cadena=\"timeout 5m ./hunalign \"+hundict+\" -utf -realign -text \\\"\"+dir1+\"/\"+file1+\"\\\" \"+\"\\\"\"+dir2+\"/\"+file2+\"\\\" > \\\"\"+dir3+\"/\"+fileali+\"\\\"\"\n",
        "        print(cadena)\n",
        "        sortida.write(cadena+\"\\n\")\n",
        "    else:\n",
        "        print(\"***\",file1)"
      ],
      "metadata": {
        "id": "1_mU7VD6tRJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Python Script to Select Hunalign Alignments Above a Specified Confidence and Merge into a Single File**"
      ],
      "metadata": {
        "id": "48tEAnDwtbsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import codecs\n",
        "import unicodedata\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "def remove_control_characters(s):\n",
        "    return \"\".join(ch for ch in s if unicodedata.category(ch)[0]!=\"C\")\n",
        "\n",
        "parser = argparse.ArgumentParser(description='A script to select hunalign aligments in a directory with a higner confidence than the given one into a single file..')\n",
        "parser.add_argument(\"--inDir\", type=str, help=\"The input dir containing the hunalign alignments.\", required=True)\n",
        "parser.add_argument(\"--outFile\", type=str, help=\"The input fule.\", required=True)\n",
        "parser.add_argument(\"--confidence\", type=float, help=\"The minimun confidence (the output confidence will be highet tnar the one provided..\", required=True)\n",
        "\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "dirin=args.inDir\n",
        "fout=args.outFile\n",
        "confidence=args.confidence\n",
        "\n",
        "sortida=codecs.open(fout,\"w\",encoding=\"utf-8\")\n",
        "\n",
        "for r, d, f in os.walk(dirin):\n",
        "    for file in f:\n",
        "        fullname=os.path.join(dirin,file)\n",
        "        \n",
        "        entrada=codecs.open(fullname,\"r\",encoding=\"utf-8\")\n",
        "\n",
        "        llistasortida=[]\n",
        "        iguals=0\n",
        "        totals=0\n",
        "        for linia in entrada:\n",
        "            try:\n",
        "                linia=linia.rstrip().replace(\"~~~\",\"\")\n",
        "                camps=linia.split(\"\\t\")\n",
        "                L1seg=camps[0].strip()\n",
        "                L1seg=\" \".join(L1seg.split())\n",
        "                L2seg=camps[1].strip()\n",
        "                L2seg=\" \".join(L2seg.split())\n",
        "                totals+=1\n",
        "                if not L1seg==\"<p>\" and L1seg==L2seg: iguals+=1\n",
        "                sim=float(camps[2])\n",
        "                if sim>=confidence and not L1seg==\"<p>\" and not L1seg==\"\" and not L2seg==\"<p>\" and not L2seg==\"\":\n",
        "                    L1seg=remove_control_characters(L1seg)\n",
        "                    L2seg=remove_control_characters(L2seg)\n",
        "                    if len(L1seg)>0 and len(L2seg)>0:\n",
        "                        cadena=L1seg+\"\\t\"+L2seg\n",
        "                        llistasortida.append(cadena)\n",
        "            except:\n",
        "                pass\n",
        "        try:\n",
        "            percent=iguals/totals\n",
        "        except:\n",
        "            percent=1\n",
        "        if percent<=0.5:\n",
        "            for cadena in llistasortida:\n",
        "                camps=cadena.split(\"\\t\")\n",
        "                sl1=camps[0]\n",
        "                #print(len(sl1),sl1)\n",
        "                sl2=camps[1]\n",
        "                toWrite=True\n",
        "                if len(sl1)>25 and sl1==sl2:\n",
        "                    toWrite=False\n",
        "                if toWrite:\n",
        "                    sortida.write(cadena+\"\\n\")\n",
        "        "
      ],
      "metadata": {
        "id": "KCKGKIU8taiz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}